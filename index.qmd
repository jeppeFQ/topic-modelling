---
title: "Topic Modeling"
encoding: "UTF-8"
execute:
  echo: false
format:
  revealjs:
    include-after-body: timer.html
    slide-number: c
    show-slide-number: print
    embed-resources: true
    self-contained-math: true
    smaller: true
    scrollable: true
    theme: dark 
    html-math-method:
      method: mathjax
      url: "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML"
    footer: "Jeppe F. Qvist | 18. marts 2025"
---

## Dagens program 

1. Introduktion til Topic Modeling med særligt fokus på LDA. 

2. LDA i praksis.

*Til denne forelæsning introduceres topic modeling. Topic models er usuperviseret maskinlæringsmodeller, der har til formål at udlede temaer af tekster baseret på ord, der går igen på tværs af tekster. Der findes forskellige algoritmer til og varianter af topic modeling, men til denne undervisningsgang fokuseres på den mest almindelige algoritme/model kaldet Latent Dirichlet allocation (LDA). Det bliver både gennemgået, hvordan modellen fungerer samt hvordan man anvender disse i Python med pakken gensim.*

::: {style="font-size: 75%;"}

- Asmussen, C. B., Møller, C. (2019). Smart literature review: A practical topic modelling approach to exploratory literature review. *Journal of Big Data*, 6(1), 1-18. 

- Blei, D. M., Ng, A. Y., Jordan, M. (2003). Latent Dirichlet Allocation. *Journal of Machine Learning Research*, 3, 993–1022. 

- Blei, D. M. (2012). Probabilistic topic models. *Communications of the ACM*, 55(4), 77–84. 

- Reed, C. (2012). *Latent Dirichlet Allocation: Towards a Deeper Understanding.* 

:::

## Topic Modeling

<!---
> *Latent Dirichlet allocation*

Topic modeling is a technique used in natural language processing and machine learning to identify topics or themes that are present in a large corpus of text.

    The goal of topic modeling is to extract the underlying structure of a document collection, by automatically identifying the main topics that are discussed in the text.
        Topic modeling algorithms work by analyzing the statistical patterns of word co-occurrence in the text, and clustering together words that tend to appear together in similar contexts. These clusters of words are then interpreted as topics, which represent the main themes or ideas that are discussed in the text.

One popular algorithm for topic modeling is Latent Dirichlet Allocation (LDA), which is based on a probabilistic model of document generation. LDA assumes that each document is a mixture of several topics, and each topic is a probability distribution over a fixed set of words.

Latent Dirichlet Allocation (LDA) is a popular probabilistic topic modeling algorithm.

    The basic idea behind LDA is to represent a collection of documents as a mixture of latent topics, where each topic is a probability distribution over words. The goal of LDA is to learn these latent topics and their associated word distributions from the documents.

    LDA assumes that each document in the collection is generated from a mixture of K topics, where K is a predefined number of topics. Each topic is represented as a probability distribution over words, and each word in a document is generated from one of the K topics according to a probability distribution.

--->

## *Topic* Modeling

> **Latent** *Dirichlet allocation*

<span class="timer" data-time="120"></span>

## Topic Modeling: *anvendelsesmuligheder*

Topic modellering er en ML metode, der kan afdække `skjulte` topics eller tematikker i en stor samling tekst dokumenter. 

- **Dokument-"klynger"**: automatisk gruppering af tekster i kategorier. 

- **Indholdsanbefaling**: find (og grupper) tekster, der ligner hinanden med henblik på abefaling. "Vi tror du vil kunne lide ... "

- **Informationssøgning**: Forbedring af søgninger, vil også at inkludere ligende tekster. 

- **Opsummeringer**: Lav overblik over hvilke topics er de mest fremkomne. 

- **Monoterering**: Automatiske overblik over trends (fx på SoMe).

## Topic Modeling: *generelle begrænsninger*

- **Tolkning**: Topics er typiske abstrakte og kræver en kvalitativ (menneskelig) evaluering. 

- **Parameter-sensitivitet**: Modeller kan være sensitive overfor hvordan vi specificere modellen (fx antallet af topics, som vi selv skal angive).

- **Bag-of-Words antagelse**: Traditionelle topic modeller arbejder med BoW representation af tekst (mere herom snart). I næste forelæsning lærer Kristian jer om, hvordan i "overkommer" dette med nyere tilgange. 

## Topic modeling (1)

> ***Usuperviseret makinlæringsmodel***

> Hvad er målet i usurperviseret ML? 

<span class="timer" data-time="180"></span>

## Topic modeling (2)

> ***Usuperviseret makinlæringsmodel***

> Hvad er målet i usurperviseret ML? 

::: {style="font-size: 75%;"}
::: {.incremental}

Pointer fra Rolfs anden forelæsning (***Hvad er machine learning?***): 

1. I usuperviseret ML er målet helt enkelt: *kompleksitetsreduktion*

2. Vi har ikke nødvendigvis et klart mål til vores algoritme, vi har i stedet et ønske om, at forstå data bedre.

3. Vi har spørgsmål som: (1) Hvordan er de ”rå” grupper i data? (2) Hvad ”stikker ud” når jeg viser min computer de her grupper af data? (3) Hvilken ”sekvens” gør mennesker, når de bliver sat i en bestemt situation?

- Usuperviserede modeller er ofte mere ”grove” [end superviserede modeller]---vi kan ikke stille et specifikt spørgsmål til dem og deres måde at løse problemet på er, ligesom ved klassisk statistik, ved at ”æde” det data vi giver og så afleverer de et svar til os.

:::
:::

## Topic modeling (3)

> Hvad er *kompleksitetsreduktion*, og hvad er formålet med det? 

<span class="timer" data-time="180"></span>

## Topic modeling (4)

> Hvad er *kompleksitetsreduktion*, og hvad er formålet med det? 

::: {style="font-size: 75%;"}
::: {.incremental}

Pointer fra Rolfs tredje forelæsning (***Avanceret klyngeanalyse og datareduktion)***: 

- I dag er konteksten `tekst` (ikke mennesker) og `iterative algoritmer` (og probalistiske modeller; *ikke* `rekursive algoritmer`), men nogle grundlæggende ideer og principper går igen:

1. Dimensioner i data er bagvedliggende faktorer, der forklarer ”ting” i data vi ellers ikke havde opdaget eller ting vi havde opdaget men fordi vi har så meget data vil tage os for evigt at forholde os til. 

2. Formålet (...) er, at vi gerne vil reducere hvor ”vildt” det hele er for os. Vi kan, teknisk set, gøre det på alle former for data, men der er forskellige måder vi kan håndtere forskellige typer af data på. [Fx:]

    - Klyngedannelse, i sin absolut simpleste form, er bare det---at skabe klynger af ”ting” så de giver mening som mere end entiteter. 

    - I sociologien [og politiologien] er det stadig at finde ”dem der”, men computeren gør det muligt at finde ”dem der” uden at have en forudindtaget ide om, hvem ”de” er. 

3. Når man taler om afstand i klyngeanalyser er det ikke en fysisk afstand, men nærmere en afstand mellem holdninger, følelser mm. [Vi kan også tale om afstande mellem tekster]

:::
:::

## Latent Dirichlet allocation (1)

LDA er en probabilistisk generativ model, der blev introduceret af `Blei, Ng, and Jordan (2003)`. Metoden modellere et dokuments latente topics, hvor hvert topic er repræsenteret af en ***fordeling over ord***. 

::: {style="font-size: 75%;"}
::: {.incremental}

- Giver overblik over et større (*tekst*)materiale ved inddeling og kategorisering; *”utility [rather than] accuracy”* (Asmussen og Møller, 2019). 

- Målet er at identificere skjulte emner i en samling af dokumenter ved at analysere ordforekomster og deres sammenhænge.

    - Intuitionen er at hvert dokument repræsenteres som en blanding af emner, og hvert emne som en blanding af ord. 

- Antagelse om at en tekst er udgjort af $K$ topics (modsat binære klassificeringer af tekster).

    - En tekst kan være $70\%$ **topic A**, $10\%$ **topic B**, $20\%$ **topic C**

- Bygger på en ”bag-of-words”-tilgang; antagelsen er at ordenes rækkefølge ikke er af betydning [?!]

- Anvendes primært på en specifik tekstsamling, hvor den estimerede model i har begrænset anvendelighed i andre tekstsamling: altså, **lav ekstern validitet**.

:::

> Spg. til teksten?

:::

## Latent Dirichlet allocation (2)

::: {.incremental}

- I modsætning til traditionel klyngeanalyse, er ideen i topic modeling at hvert dokument består af flere forskellige topics i forskellige proportioner, fremfor at hvert dokument typisk kun tilhører én klynge. 

- Et dokument kan derfor tilhøre flere emner på én gang, med forskellig styrke eller sandsynlighed.

- En form for "specialiseret klyngeanalyse". 

::: 

## Latent Dirichlet allocation (3)

::: {.columns} 

::: {.column width="70%"}

![](Picture 2.png)

:::

::: {.column width="30%"}

::: {style="font-size: 65%;"}

- `Ord`, $w$: De unikke ord, der fremgår i vores samlede tekstmateriale (*vocabular*). 

- `Documenter`, $\textbf{w}$: En sekvens af $N$ ord, $\{ w_{1}, \dots, w_{N} \}$. 

- `Corpus`, $M$: En samling dokumenter, $\{ \textbf{w}_{1}, \dots, \textbf{w}_{N} \}$. 

- `Topics`, $z_{1:K}$: Hvert topic er udtrykt som en sandsynlighedsfordeling af ord, $w$.

- `Topic fordeling`, $\phi_{w,k}$: Andelen af topic $k$ i dokument $\textbf{w}$.

- `emne-tildelingen for ord`, $\theta_{\textbf{w},w}$: Tildeling af topic $k$ til ord $w$ i tekst $\textbf{w}$.

- `Bag-of-Word`: Vektoriceret (simplificeret) repræsentation af en tekst som en optælling af ord, der fremkommer. Ignorerer grammatik, kontekst, og rækkefølge af ord. 

:::

:::

:::

## Latent Dirichlet allocation (4)

Intuitionen er at hvert dokument er "skabt" på baggrund af fordelingen af topics i teksten. Vi vil udlede den sandsynlighedsmodel, der (teoretisk) kunne have "skabt teksten".

::: {.incremental}

- Hvert topic er repræsenteret som sandsynligheden for at hvert ord er del af det pågældende topic. 

    > **Hvis en tekst** (*repræsenteret som BoW*) **er $80\%$ "politik", $15\%$ "krig" og $5\%$ "forskning", ville en korrekt specificeret model (teoretisk) kunne genskabe denne BoW, ved at vælge $80\%$ "politik-ord", $15\%$ "krigs-ord" og $5\%$ "forsknings-ord", hvis vi gender de underliggende fordelinger af topics of ord** 

- Teknisk vil vi fra vores corpus udlede de skjulte fordelinger: 

    1. `Topic-word` fordelingen: *hvor sandsynligt det er for specifikke ord at optræde i hvert topic*

    2. `Document-topic` fordelingen: *hvor fremtræden hvert topic er i hvert dokument*.

:::

## Latent Dirichlet allocation (5)

> Når jeg siger at hvert *topic* i LDA er en "fordeling", hvad tror I jeg mener med det?

::: {.incremental}

- Hvert topic er "blot" en samling af ord, hvor hvert ord har en estimeret sandsynlighed, der indikerer hvor sandsynlige deres fremkomst er i en tekst med et givent topic. 

- Topics kan også tænkes i "bag"-analogien: hvert tema er en "bag" af ord, hvor nogle er meget fremkomne i én "bag", men mindre sandsynlige i en anden "bag". 

    - "Klima" er meget sandsynlig i vores *"miljø"*-topic, men fremkommer også i vores *"skole"*-topic, endog fremkomsten af ordet her er mindre sandsynligt en det første topic. 

:::

<span class="timer" data-time="120"></span>

## Latent Dirichlet allocation (5)

> **Artikel**: Regeringen med minister Y i spidsen præsenterer nye målsætninger for reducering af klimapåvirkning fra landbruget. 

1. Topic 1: *Politik* ($60\%$): regering, målsætning*, minister, reducering, landbrug, $\dots$

2. Topic 2: *Klima* ($40\%$): klima*, landbrug, reducering, $\dots$ 

::: {style="font-size: 65%;"}

| Ord            | Sandsynlighed (Hvor ofte?) | Forklaring                                   |
|----------------|-----------------------------|---------------------------------------------|
| Klima          | Høj                         | Ofte centralt i diskussioner                |
| Forurening     | Høj                         | Hyppigt omtalt miljøproblem                 |
| Genbrug        | Middel-høj                  | Almindeligt, men mindre centralt end "klima" |
| Bæredygtighed  | Middel                      | Vigtigt, men mindre hyppigt                 |
| Plastik        | Middel                      | Ofte relateret til affald og forurening     |
| Vedvarende     | Middel-lav                  | Forekommer i energirelaterede diskussioner  |
| Økosystem      | Middel-lav                  | Vigtigt, men specialiseret                  |
| Udledninger    | Lav                         | Teknisk, specifikke sammenhænge             |
| Solenergi      | Lav                         | Mere specialiseret del af miljøområdet      |

: Hyppighed af ord i Klima-topic {tbl-cap}

:::

## Inferens: hvordan "lærer" LDA (1)

::: {.columns} 

::: {.column width="70%"}

![](Picture 2.png)

:::

::: {.column width="30%"}

::: {style="font-size: 65%;"}

::: {.incremental}

- Vi vil udlede den sandsynlighedsmodel, der (teoretisk) kunne have "skabt teksten".

- Konceptuelt, og i praksis, vil vi "reverse-engineer" denne generative proces, der kunne have skabt teksten, ved at estimere de skjulte fordelinger i data: 

    1. `Topic-word` fordelingen, $\phi$: *hvor sandsynligt det er for specifikke ord at optræde i hvert topic*

    2. `Document-topic` fordelingen, $\theta$: *hvor fremtræden hvert topic er i hvert dokument*.

    3. Vi bruger `Gibbs sampling` til at estimere disse fordelinger. 

:::

:::

:::

:::

## Inferens: hvordan "lærer" LDA (2)

LDA er en `iterativ algoritme`, der tilskriver topics ved:  

::: {.incremental}

::: {style="font-size: 90%;"}

1. Hvor almindeligt/fremtrædende er et topic, $k$, i et givent dokument, $\textbf{w}$ (`Document-topic proportion`)

2. Hvor sandsynligt er det at et ord, $w$, er en del at topic, $k$, på tværs af hele corpusset $M$ (`Topic-word likelihood`)

- Algoritmen giver to outouts: 

    1. `Topic-word` fordelingen, $\phi$: som dermed viser hvilket ord, $w$, definere hvert topic, $k$. 

    2. `Document-topic` fordelingen, $\theta$: som viser sammensætningerne af topics, $K$, for hvert dokument, $\textbf{w}$.

- Vi har nu estimeret en model, der kan: 

    1. Klassificere/tilskrive topics til en tekst. 

    2. Afdække de underlæggende topics i en samling dokumenter. 

    3. Lave semantisk/tematisk "clustering" af dokumenter, der muliggør flere af anvendelserne, præsenteret i starten. 

:::

:::

## LDA algoritmen (1.1)

::: {style="font-size: 75%;"}

LDA er iterativ, idet det er en proces, hvor algoritmen laver en gradvis "forbedring" af resultaterne (fordelingerne)---gører tilpasningsdelen igen og igen---indtil den finder en stabil løsning (konvergerer), hvor hver gentagelse kun ændrer resultaterne minimalt. Ved dette punkt har algoritmen har fundet et stabilt sæt af topics. 

```

~~Stiliseret iterativ algoritme~~ 
Input: Et sæt af dokumenter D, antal ønskede topics K
Output: Document-topic-fordeling og Topic-word-fordeling

Start:
    Initialisér tilfældigt et topic for hvert ord i alle dokumenter

    Gentag
        For hvert dokument W i M:
            For hvert ord w i dokument W:
                Fjern det nuværende topic k for ordet w

                For hvert topic k fra 1 til K:
                    Beregn sandsynlighed P(k) baseret på:
                        a. Hvor hyppigt topic k optræder i dokument W (Document-topic-fordeling)
                        b. Hvor sandsynligt ordet w er under topic k (Topic-word-fordeling)

                Vælg nyt topic k' til ordet w baseret på sandsynlighedsfordelingen P(k)

                Tildel det nye topic k' til ordet w

        Opdater Document-topic-fordeling og Topic-word-fordeling baseret på nye topic-tildelinger

    Indtil (konvergens nås: ændringerne mellem iterationer er minimale)

Returnér Document-topic-fordeling og Topic-word-fordeling
Slut

```

:::

## LDA algoritmen (1.2)

**Hvordan virker algoritmen? (e.g., p. 1005 i Blei et al., 2003): *Inferens af posteriour distributions*.**

::: {style="font-size: 50%;"}

Suppose you’ve just moved to a new city. You’re a hipster and an anime fan, so you want to know where the other hipsters and anime geeks tend to hang out. Of course, as a hipster, you know you can’t just ask, so what do you do?

Here’s the scenario: you scope out a bunch of different establishments (documents) across town, making note of the people (words) hanging out in each of them (e.g., Alice hangs out at the mall and at the park, Bob hangs out at the movie theater and the park, and so on). Crucially, you don’t know the typical interest groups (topics) of each establishment, nor do you know the different interests of each person.

So you pick some number K of categories to learn (i.e., you want to learn the K most important kinds of categories people fall into), and start by making a guess as to why you see people where you do. For example, you initially guess that Alice is at the mall because people with interests in X like to hang out there; when you see her at the park, you guess it’s because her friends with interests in Y like to hang out there; when you see Bob at the movie theater, you randomly guess it’s because the Z people in this city really like to watch movies; and so on.

Of course, your random guesses are very likely to be incorrect (they’re random guesses, after all!), so you want to improve on them. One way of doing so is to:

- Pick a place and a person (e.g., Alice at the mall).

- Why is Alice likely to be at the mall? Probably because other people at the mall with the same interests sent her a message telling her to come.

- In other words, the more people with interests in X there are at the mall and the stronger Alice is associated with interest X (at all the other places she goes to), the more likely it is that Alice is at the mall because of interest X.

- So make a new guess as to why Alice is at the mall, choosing an interest with some probability according to how likely you think it is.

Go through each place and person over and over again. Your guesses keep getting better and better (after all, if you notice that lots of geeks hang out at the bookstore, and you suspect that Alice is pretty geeky herself, then it’s a good bet that Alice is at the bookstore because her geek friends told her to go there; and now that you have a better idea of why Alice is probably at the bookstore, you can use this knowledge in turn to improve your guesses as to why everyone else is where they are), and eventually you can stop updating. Then take a snapshot (or multiple snapshots) of your guesses, and use it to get all the information you want:

- For each category, you can count the people assigned to that category to figure out what people have this particular interest. By looking at the people themselves, you can interpret the category as well (e.g., if category X contains lots of tall people wearing jerseys and carrying around basketballs, you might interpret X as the “basketball players” group).

- For each place P and interest category C, you can compute the proportions of people at P because of C (under the current set of assignments), and these give you a representation of P. For example, you might learn that the people who hang out at Barnes & Noble consist of 10% hipsters, 50% anime fans, 10% jocks, and 30% college students.

[*OPRINDELIG TEKST*](blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/) 

:::

## LDA algoritmen (2)

Vi bruger en afart af `Bayes Theorem` til LDA, hvor vi opdaterer modellens parametre ($\theta, \phi, z$), ved at observere data, ordene, $w$:

$$
P(\theta, \phi, z \mid w, \alpha, \beta) = \frac{P(w \mid z, \phi)P(z \mid \theta)P(\theta \mid \alpha)P(\phi \mid \beta)}{P(w \mid \alpha, \beta)}
$$

Hvor, 

- $w$ er observerede ord.

- $z$ er latente topics.

- $\theta$ er `Document-topic` fordelingen.

- $\phi$ er `Topic-word` fordelingen. 

- $\alpha, \beta$ er `hyperparametre`. 

<span class="timer" data-time="180"></span>

## LDA algoritmen (3.1)

Vi bruger en afart af `Bayes Theorem` til LDA, hvor vi opdaterer modellens parametre ($\theta, \phi, z$), ved at observere data, ordene, $w$:

$$
P(\theta, \phi, z \mid w, \alpha, \beta) = \frac{P(w \mid z, \phi)P(z \mid \theta)P(\theta \mid \alpha)P(\phi \mid \beta)}{P(w \mid \alpha, \beta)}
$$

::: {.incremental}

- $P(w|z, \phi)$ er sandsynligheden for at observere ordene $w$, hvis vi kender topic-tildelingerne $z$ og topic-word fordelingerne $\phi$.

    - <p><span style="font-size:0.7em">*Når vi ved, hvilke topics ord kommer fra, og hvordan topics "genererer" ord, kan vi beregne sandsynligheden for ordene i dokumenterne.*</span></p>

- $P(z|\theta)$ er sandsynligheden for at hvert ord får en bestemt topic-tildeling $z$, givet dokumenternes emnefordeling $\theta$.

    - <p><span style="font-size:0.7em">*Hvis vi ved, hvor meget hvert topic fylder i dokumentet, kan vi beregne sandsynligheden for, at ordet tilhører et bestemt topic.*</span></p>

:::

## LDA algoritmen (3.2)

Vi bruger en afart af `Bayes Theorem` til LDA, hvor vi opdaterer modellens parametre ($\theta, \phi, z$), ved at observere data, ordene, $w$:

$$
P(\theta, \phi, z \mid w, \alpha, \beta) = \frac{P(w \mid z, \phi)P(z \mid \theta)P(\theta \mid \alpha)P(\phi \mid \beta)}{P(w \mid \alpha, \beta)}
$$

::: {.incremental}

- $P(\theta|\alpha)$ er sandsynligheden for document-topic fordelingen $\theta$, givet *Dirichlet-fordelingen* med hyperparametre $\alpha$.

    - <p><span style="font-size:0.7em">*Vi starter med en antagelse om, hvordan topics typisk fordeler sig i dokumenter (bestemt af \alpha). Denne antagelse justeres iterativt, når vi ser på data.*</span></p>

- $P(\psi|\beta)$ er sandsynligheden for topic-word fordelingen $\psi$, givet *Dirichlet-fordelingen* med hyperparametre $\beta$.

    - <p><span style="font-size:0.7em">*Vi starter med en antagelse om, hvordan ordene typisk fordeler sig i topics (bestemt af $\beta$). Denne antagelse justeres iterativt ud fra data.*</span></p>

:::

## LDA algoritmen (3.3)

Vi bruger en afart af `Bayes Theorem` til LDA, hvor vi opdaterer modellens parametre ($\theta, \phi, z$), ved at observere data, ordene, $w$:

$$
P(\theta, \phi, z \mid w, \alpha, \beta) = \frac{P(w \mid z, \phi)P(z \mid \theta)P(\theta \mid \alpha)P(\phi \mid \beta)}{P(w \mid \alpha, \beta)}
$$

::: {.incremental}

- $P(w|\alpha, \beta)$ er en *normaliseringsfaktor*, der sikrer at sandsynlighederne summerer til 1.

:::

## LDA algoritmen (3.4)

Vi bruger en afart af `Bayes Theorem` til LDA, hvor vi opdaterer modellens parametre ($\theta, \phi, z$), ved at observere data, ordene, $w$:

$$
P(\theta, \phi, z \mid w, \alpha, \beta) = \frac{P(w \mid z, \phi)P(z \mid \theta)P(\theta \mid \alpha)P(\phi \mid \beta)}{P(w \mid \alpha, \beta)}
$$

Formlen beskriver, hvordan LDA bruger data (ordene i teksterne) og prior antagelser (hyperparametrene) til iterativt at finde og opdatere topics og deres fordelinger.

Den gør dette iterativt, indtil ***sandsynligheden for at observere vores data*** (givet de modellerede topis) bliver maksimal---og resultatet bliver en meningsfuld ***inddeling af store tekster i skjulte topics***.

Grundlæggende vil vi finde de værdier for $\theta$, $\phi$ og $z$, der gør, at sandsynligheden for de pågældende topic-inddelinger bliver størst muligt (maksimering af ***posterior-sandsynligheden***).

## LDA algoritmen (3.5)

Et hypotetisk output: 

::: {.incremental}

- `Document-topic` fordeling, $\theta$: Én tekst består af $70\%$ "***politik***"-topic, $20\%$ "***økonomi***"-topic og $10\%$ "***teknologi***"-topic.

- `Topic-word` fordeling, $\phi$: "***Politik***"-topic er kendetegnet ved ord som "regering", "valg", "minister". "***Økonomi***"-topic ved ord som "inflation", "arbejdsløshed", "vækst".

::: {style="font-size: 75%;"}

- LDA algoritmen starter med et tilfældigt startantagelse---"gæt"---vedr. disse fordelinger (ord i tilfælde topics, topic fordelinger i tekster); disse priors opdateres iterativt, $n$ antal gange (hundredevis, tusindvis). 

    - For hver iteration, bliver "gættet" bedre; kommer tættere på en model, der kunne have skabt den pågældende tekst (i.e., "reverse-engineering af den 'ægte' model"). 

    - Iteration efter iteration opdaterer algoritmen fordelingerne af topics i dokumenter ($\theta$) og fordelinger af ord i topics ($\phi$).

    - Teksterne stabiliserer sig i `topic-proportioner`, og topics stabiliserer sig med `ord-proportioner`, der passer bedst med dataen. 

    - Disse proportioner giver os endeligt meningsfulde topics med meningsfulde ord (topic-word fordelingen) og en bestemmelse af proportionerne af topics i hver tekst (document-topic fordelingen).

    - Spg.: ***Hvad vil "meningsfuld" i denne kontekst sige?***

:::

:::

## L`D`A algoritmen (3.6)

::: {style="font-size: 75%;"}

I LDA er vores prior, startantagelse om fordelingerne, en `Dirichlet`-fordeling. 

$$
f(\theta; \alpha) = \frac{\Gamma\left(\sum_{i=1}^{K}\alpha_i\right)}{\prod_{i=1}^{K}\Gamma(\alpha_i)} \prod_{i=1}^{K}\theta_i^{\alpha_i - 1}
$$

::: {.incremental}

- Dirichlet-fordelingen udtrykker fordelinger over (vektorer af) proportioner (der summerer til 1).

    - Fx $70\%$ "***politik***"-topic, $20\%$ "***økonomi***"-topic og $10\%$ "***teknologi***"-topic $= 100\%$ topics. 

- Når vi med LDA modellerer sandsynligheder eller proportioner for topics *i* dokumenter og ord *i* topics, *skal* proportionerne opfylde: 

    1. *Alle proportioner er positive eller nul.*

    2. *Alle proportioner skal summere til $1$*

- Vi bruger `Dirichlet`-fordelingen to steder: 

    1. `Document-topic` fordelingen ($\theta$): For hver tekst $\textbf{w}$ vælges en fordeling over topics fra en Dirichlet-fordeling med parameter $\alpha$, $\theta_{\textbf{w}} \sim \text{Dirichlet}(\alpha)$

        - <p><span style="font-size:0.7em">*LDA starter med at gætte en tilfældigt fordeling af topics i hver tekst, styret af $\alpha$*</span></p>

    2. `Topic-word` fordelingen ($\phi$): For hvert topic vælges en fordeling over ord fra en Dirichlet-fordeling med parameter $\beta$: $\phi_{k} \sim \text{Dirichlet}(\beta)$

        - <p><span style="font-size:0.7em">*LDA starter med at en tilfældigt fordeling af ord i hvert topic, styret af $\beta$*</span></p>
:::

:::

## L`D`A algoritmen (3.7)

::: {style="font-size: 75%;"}

I LDA er vores prior, startantagelse om fordelingerne, en `Dirichlet`-fordeling. 

$$
f(\theta; \alpha) = \frac{\Gamma\left(\sum_{i=1}^{K}\alpha_i\right)}{\prod_{i=1}^{K}\Gamma(\alpha_i)} \prod_{i=1}^{K}\theta_i^{\alpha_i - 1}
$$

Parametrene $\alpha_{i}$ er vores "prior count" for topics (som vi selv vælger) og bestemmer graden af variation i fordeling af topics: 

::: {.incremental}

- $\alpha < 1$: Sandsynlighedsfordelingen koncentreres omkring få topics. Ét eller få topics dominerer i tekster og corpus.

- $\alpha = 1$: Topics er omtrent ens sandsynlige. Tilfældige og jævne topic-fordelinger i tekster og corpus.

- $\alpha > 1$: Jævn fordeling på tværs af alle topics. Tekster har ofte en ligelig blanding af mange topics.

<!---
    For, the higher values of alpha —> the documents will be composed of more topics, and
    The lower values of alpha —> returns documents with fewer topics.

Similarly, for the values of Beta:

    The higher beta —> has more number of words in a given topic, and
    The lower value of beta —> topics contains few words.
---->

:::

::: 

## L`D`A algoritmen (3.8)

I LDA er vores prior, startantagelse om fordelingerne, en `Dirichlet`-fordeling. 

$$
f(\theta; \alpha) = \frac{\Gamma\left(\sum_{i=1}^{K}\alpha_i\right)}{\prod_{i=1}^{K}\Gamma(\alpha_i)} \prod_{i=1}^{K}\theta_i^{\alpha_i - 1}
$$

Hypotetiske $\alpha$ parametre for et corpus med 3 topics (Politik, økonomi, klima): 

::: {.incremental}

1. $\alpha = [1, 1, 1]$

    - Emnerne er lige sandsynlige, fx: $(0.33, 0.33, 0.33)$, $(0.4, 0.3, 0.3)$, $(0.3, 0.3, 0.4)$.

2. $\alpha = [0.1, 0.1, 0.1]$

    - Meget sparsomme fordelinger med dominans af ét emne, fx: $(0.95, 0.04, 0.01)$.

3. $\alpha = [10, 10, 10]$

    - Meget jævne fordelinger hvor ingen topics er klart dominerende, fx: $(0.34, 0.33, 0.33)$.

:::

## Fordele og ulemper (1)

<span style="color: green;">Fordele</span>

::: {.incremental}

::: {style="font-size: 100%;"}

- Topics er generelt ligefremme at tolke og "gennemsigtige", da de bliver bestemt som en samling ord og deres tilknyttede sandsynligheder, der kan tilgås. 

- Hyperparametrene/mekanismerne, der påvirker output ($\alpha$, $\beta$), er eksplicitte og kan tilgås. 

- "Nem", effektiv med mange veludviklede biblioteker (`Gensim`, `MALLET`, `Scikit-learn`), og skalerer godt (med Gibbs sampling); også med store datasæt. 

- Fungerer uproblematisk i ustruktureret tekst med støj. 

- Den veletablerede matematiske ramme gør metode og resultater (relativt) nemme at forklare og argumenterer for (eller imod). 

:::

:::

> <span style="color: green;">Hvad ser *I* som fordelene ved topic modeling (med LDA)?</span>

<span class="timer" data-time="120"></span>

## Fordele og ulemper (2)

<span style="color: red;">Ulemper</span>

::: {.incremental}

::: {style="font-size: 87%;"}

- Lav grad af "*kontekstforståelse*", da det hele er baseret på en `bag-of-words` repræsentation af tekst; ikke ordstilling eller semantik. Konsekvensen kan være upræcise eller meningsløse topics. 

- Klarer sig generelt dårligt på korte tekster, da det er en probabilistisk model og korte tekster giver meget lidt statistisk belæg; dvs. den performer dårligt på fx tweets. 

- Fordi topics er baseret på statistik frem for mening kan LDA give os topics, der statistisk giver mening, men semantisk er uklare. *Det er altså en teknisk forståelse af "mening", eller substantiel eller teoretisk.*

- Håndterer *polysemi* (ord med flere betydninger) og tvetydighed dårligt og kan påvirker meningen i topics negativt. 

- Valget af antallet af topics og hyperparametrene, kræver en "eksperimentierende" tilgang (og erfaring). *Her kan i med jeres teoretiske viden bidrage med noget som "rene" dataloger ikke nødvendigvis kan*.

:::

::: 

> <span style="color: red;">Hvad ser *I* som ulemperne ved topic modeling (med LDA)?</span>

<span class="timer" data-time="120"></span>

## Word embedding (næste gang med Kristian)

<span style="color: red;">Hvis LDA vurderes utilstrækkelig?</span>

Hvis vores tekster er korte, eller det bliver tydeligt at semantik og kontekst er særligt centralt for vores specifikke corpus, kan vi overveje `word embedding` metoder (fx Word2Vec, BERT), fordi denne tilgang, bl.a.: 

- Bevarer semantisk kontekst. 

- Håndterer polysemi bedre. 

- Fungerer bedre med korte tekster.

# LDA i praksis

## Forventet udbytte

- Arbejde med vektoriseret tekst i Python (opsummering?)

- Implementering af LDA i Pythin

## Vektorisering af tekst til `Bag-of-Words`

Preprocessing: 

1. Fjern stopord
2. Fjern tegnsætning
3. Fjern tal 
4. Lemmatization

- *Hvad mere?*

::: {.incremental}

- Konstruér `document-term-matrix` (DTM)

:::

<span class="timer" data-time="120"></span>

## `Gensim` implementering af LDA

Base model: 

```
LdaModel eller LdaMulticore
```

```
    Input:
        Document-term matrix: Bag-Of-Words (BOW) format.
        Number of topics: K
        Numper of iterations [epochs]: N
        alpha, document-topic density: 'auto' 
        eta [beta], topic word density: 'auto' 
    Output: the top words in each topic
```

## Evaluering af LDA og `hyperparameter tuning` (1)

<!----

Natural language is messy, ambiguous and full of subjective interpretation, and sometimes, trying to cleanse ambiguity reduces the language to an unnatural form. In this article, we’ll explore more about topic coherence, an intrinsic evaluation metric, and how you can use it to quantitatively justify the model selection.

We know probabilistic topic models, such as LDA, are popular tools for text analysis, providing both a predictive and latent topic representation of the corpus. However, there is a longstanding assumption that the latent space discovered by these models is generally meaningful and useful, and that evaluating such assumptions is challenging due to its unsupervised training process. Besides, there is a no-gold standard list of topics to compare against every corpus.

Nevertheless, it is equally important to identify if a trained model is objectively good or bad, as well have an ability to compare different models/methods. To do so, one would require an objective measure for the quality. Traditionally, and still for many practical applications, to evaluate if “the correct thing” has been learned about the corpus, an implicit knowledge and “eyeballing” approaches are used. Ideally, we’d like to capture this information in a single metric that can be maximized, and compared.

Eyeballing Models

    Top N words
    Topics/documents

Intrinsic evaluation metrics

    Capturing model semantics
    Topics interpretability

Human judgements

    What is a topic?

Extrinsic evaluation metrics/evaluation at task

    Is the model good at performing predefined tasks, such as classification?



What is topic coherence?

Before we understand topic coherence, let’s briefly look at the perplexity measure. Perplexity as well is one of the intrinsic evaluation metric, and is widely used for language Model Evaluation. It captures how surprised a model is by new data it has not seen before and is measured as the normalized log-likelihood of a held-out test set.

Focussing on the log-likelihood part, you can think of the perplexity metric as measuring how probable some new unseen data is given the model that was learned earlier. That is to say, how well does the model represent or reproduce the statistics of the held-out data?

However, recent studies have shown that predictive likelihood (or equivalently, perplexity) and human judgment are often not correlated, and even sometimes slightly anti-correlated.

    Optimizing for perplexity may not yield human-interpretable topics.

This limitation of perplexity measure served as a motivation for more work trying to model the human judgment and, thus, Topic Coherence.

The concept of topic coherence combines a number of measures into a framework to evaluate the coherence between topics inferred by a model. But before that…

What is topic coherence?

Topic Coherence measures score a single topic by measuring the degree of semantic similarity between high scoring words in the topic. These measurements help distinguish between topics that are semantically interpretable topics and topics that are artifacts of statistical inference. But …

What is coherence?

A set of statements or facts is said to be coherent, if they support each other. Thus, a coherent fact set can be interpreted in a context that covers all or most of the facts. An example of a coherent fact set is “the game is a team sport”, “the game is played with a ball”, “the game demands great physical efforts.”
Coherence measures

Let’s take quick look at different coherence measures, and how they are calculated:

    C_v measure is based on a sliding window, one-set segmentation of the top words and an indirect confirmation measure that uses normalized pointwise mutual information (NPMI) and the cosine similarity
    C_p is based on a sliding window, one-preceding segmentation of the top words and the confirmation measure of Fitelson’s coherence
    C_uci measure is based on a sliding window and the pointwise mutual information (PMI) of all word pairs of the given top words
    C_umass is based on document cooccurrence counts, a one-preceding segmentation and a logarithmic conditional probability as confirmation measure
    C_npmi is an enhanced version of the C_uci coherence using the normalized pointwise mutual information (NPMI)
    C_a is baseed on a context window, a pairwise comparison of the top words and an indirect confirmation measure that uses normalized pointwise mutual information (NPMI) and the cosine similarity

There is, of course, a lot more to the concept of topic model evaluation, and the coherence measure. However, keeping in mind the length, and purpose of this article, let’s apply these concepts into developing a model that is at least better than with the default parameters. Also, we’ll be re-purposing already available online pieces of code to support this exercise instead of re-inventing the wheel.

---->

## Evaluering af LDA og `hyperparameter tuning` (2)


<!----

Evaluation¶

Evaluating an LDA (Latent Dirichlet Allocation) model is an important step in topic modeling to determine the effectiveness of the model in discovering underlying topics in a corpus of text documents. There are several methods for evaluating an LDA model, including:

    Perplexity: Perplexity is a commonly used metric for evaluating LDA models. It measures how well the model predicts new data. A lower perplexity score indicates better performance. You can calculate perplexity using the log_perplexity() method of the LDA model in gensim.

    Coherence: Coherence measures the degree of semantic similarity between the top N words in each topic. Higher coherence scores indicate better topic quality. You can calculate coherence using the CoherenceModel class in gensim.

    Visualization: Visualization can be used to visually inspect the quality of the topics generated by the LDA model. You can use the pyLDAvis package in Python to create interactive visualizations of the topics.

    Human evaluation: Finally, you can also perform human evaluation by having human annotators review the topics generated by the LDA model and provide feedback on the quality and interpretability of the topics.

In summary, evaluating an LDA model involves using one or more of the above methods to measure the quality of the topics generated by the model. By evaluating the model, you can make adjustments to the parameters and improve the quality of the topics.




Hyperparameter Tuning¶

First, let’s differentiate between model hyperparameters and model parameters :

    Number of Topics (K)
    Dirichlet hyperparameter alpha: Document-Topic Density
    Dirichlet hyperparameter beta: Word-Topic Density


Hyperparameter tuning

First, let’s differentiate between model hyperparameters and model parameters:

    Model hyperparameters can be thought of as settings for a machine learning algorithm that are tuned by the data scientist before training. Examples would be the number of trees in the random forest, or in our case, number of topics K

    Model parameters can be thought of as what the model learns during training, such as the weights for each word in a given topic

Now that we have the baseline coherence score for the default LDA model, let’s perform a series of sensitivity tests to help determine the following model hyperparameters:

    Number of topics (K)
    Dirichlet hyperparameter alpha: Document-topic density
    Dirichlet hyperparameter beta: Word-topic density

We’ll perform these tests in sequence, one parameter at a time by keeping others constant and run them over the two different validation corpus sets. We’ll use C_v as our choice of metric for performance comparison.

--->

## Visualisering af LDA 

<!----




---->