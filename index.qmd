---
title: "Topic Modeling"
encoding: "UTF-8"
execute:
  echo: false
format:
  revealjs:
    slide-number: c
    show-slide-number: print
    embed-resources: true
    self-contained-math: true
    smaller: true
    scrollable: true
    theme: dark 
    html-math-method:
      method: mathjax
      url: "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML"
    footer: "Jeppe F. Qvist | 18. marts 2025"
---

## Dagens program 

1. Introduktion til Topic Modeling med s√¶rligt fokus p√• LDA. 

2. LDA i praksis.

*Til denne forel√¶sning introduceres topic modeling. Topic models er usuperviseret maskinl√¶ringsmodeller, der har til form√•l at udlede temaer af tekster baseret p√• ord, der g√•r igen p√• tv√¶rs af tekster. Der findes forskellige algoritmer til og varianter af topic modeling, men til denne undervisningsgang fokuseres p√• den mest almindelige algoritme/model kaldet Latent Dirichlet allocation (LDA). Det bliver b√•de gennemg√•et, hvordan modellen fungerer samt hvordan man anvender disse i Python med pakken gensim.*

::: {style="font-size: 75%;"}

- Asmussen, C. B., M√∏ller, C. (2019). Smart literature review: A practical topic modelling approach to exploratory literature review. *Journal of Big Data*, 6(1), 1-18. 

- Blei, D. M., Ng, A. Y., Jordan, M. (2003). Latent Dirichlet Allocation. *Journal of Machine Learning Research*, 3, 993‚Äì1022. 

- Blei, D. M. (2012). Probabilistic topic models. *Communications of the ACM*, 55(4), 77‚Äì84. 

- Reed, C. (2012). *Latent Dirichlet Allocation: Towards a Deeper Understanding.* 

:::

## Topic Modeling

> *Latent Dirichlet allocation*

## *Topic* Modeling

> **Latent** *Dirichlet allocation*

## Topic Modeling: *anvendelsesmuligheder*

Topic modellering er en ML metode, der kan afd√¶kke `skjulte` topics eller tematikker i en stor samling tekst dokumenter. 

- **Dokument-"klynger"**: automatisk gruppering af tekster i kategorier. 

- **Indholdsanbefaling**: find (og grupper) tekster, der ligner hinanden med henblik p√• abefaling. "Vi tror du vil kunne lide ... "

- **Informationss√∏gning**: Forbedring af s√∏gninger, vil ogs√• at inkludere ligende tekster. 

- **Opsummeringer**: Lav overblik over hvilke topics er de mest fremkomne. 

- **Monoterering**: Automatiske overblik over trends (fx p√• SoMe).

## Topic Modeling: *generelle begr√¶nsninger*

- **Tolkning**: Topics er typiske abstrakte og kr√¶ver en kvalitativ (menneskelig) evaluering. 

- **Parameter-sensitivitet**: Modeller kan v√¶re sensitive overfor hvordan vi specificere modellen (fx antallet af topics, som vi selv skal angive).

- **Bag-of-Words antagelse**: Traditionelle topic modeller arbejder med BoW representation af tekst (mere herom snart). I n√¶ste forel√¶sning l√¶rer Kristian jer om, hvordan i "overkommer" dette med nyere tilgange. 

## Topic Modeling

> ***Usuperviseret makinl√¶ringsmodel***

## Topic modeling (1)

> ***Usuperviseret makinl√¶ringsmodel***

> Hvad er m√•let i usurperviseret ML? 

## Topic modeling (2)

> ***Usuperviseret makinl√¶ringsmodel***

> Hvad er m√•let i usurperviseret ML? 

::: {style="font-size: 75%;"}
::: {.incremental}

Pointer fra Rolfs anden forel√¶sning (***Hvad er machine learning?***): 

1. I usuperviseret ML er m√•let helt enkelt: *kompleksitetsreduktion*

2. Vi har ikke n√∏dvendigvis et klart m√•l til vores algoritme, vi har i stedet et √∏nske om, at forst√• data bedre.

3. Vi har sp√∏rgsm√•l som: (1) Hvordan er de ‚Äùr√•‚Äù grupper i data? (2) Hvad ‚Äùstikker ud‚Äù n√•r jeg viser min computer de her grupper af data? (3) Hvilken ‚Äùsekvens‚Äù g√∏r mennesker, n√•r de bliver sat i en bestemt situation?

- Usuperviserede modeller er ofte mere ‚Äùgrove‚Äù [end superviserede modeller]---vi kan ikke stille et specifikt sp√∏rgsm√•l til dem og deres m√•de at l√∏se problemet p√• er, ligesom ved klassisk statistik, ved at ‚Äù√¶de‚Äù det data vi giver og s√• afleverer de et svar til os.

:::
:::

## Topic modeling (3)

> Hvad er *kompleksitetsreduktion*, og hvad er form√•let med det? 

## Topic modeling (4)

> Hvad er *kompleksitetsreduktion*, og hvad er form√•let med det? 

::: {style="font-size: 75%;"}
::: {.incremental}

Pointer fra Rolfs tredje forel√¶sning (***Avanceret klyngeanalyse og datareduktion)***: 

- I dag er konteksten `tekst` (ikke mennesker) og `iterative algoritmer` (probalistiske modeller; ikke rekursive algoritmer), men nogle grundl√¶ggende ideer og principper g√•r igen:

1. Dimensioner i data er bagvedliggende faktorer, der forklarer ‚Äùting‚Äù i data vi ellers ikke havde opdaget eller ting vi havde opdaget men fordi vi har s√• meget data vil tage os for evigt at forholde os til. 

2. Form√•let (...) er, at vi gerne vil reducere hvor ‚Äùvildt‚Äù det hele er for os. Vi kan, teknisk set, g√∏re det p√• alle former for data, men der er forskellige m√•der vi kan h√•ndtere forskellige typer af data p√•. [Fx:]

    - Klyngedannelse, i sin absolut simpleste form, er bare det---at skabe klynger af ‚Äùting‚Äù s√• de giver mening som mere end entiteter. 

    - I sociologien [og politiologien] er det stadig at finde ‚Äùdem der‚Äù, men computeren g√∏r det muligt at finde ‚Äùdem der‚Äù uden at have en forudindtaget ide om, hvem ‚Äùde‚Äù er. 

3. N√•r man taler om afstand i klyngeanalyser er det ikke en fysisk afstand, men n√¶rmere en afstand mellem holdninger, f√∏lelser mm. [Vi kan ogs√• tale om afstande mellem tekster]

:::
:::

## Latent Dirichlet allocation (1)

LDA er en probabilistisk generativ model, der blev introduceret af `Blei, Ng, and Jordan (2003)`. Metoden modellere et dokuments latente topics, hvor hvert topic er repr√¶senteret af en ***fordeling over ord***. 

::: {style="font-size: 75%;"}
::: {.incremental}

- Giver overblik over et st√∏rre (*tekst*)materiale ved inddeling og kategorisering; *‚Äùutility [rather than] accuracy‚Äù* (Asmussen og M√∏ller, 2019). 

- M√•let er at identificere skjulte emner i en samling af dokumenter ved at analysere ordforekomster og deres sammenh√¶nge.

    - Intuitionen er at hvert dokument repr√¶senteres som en blanding af emner, og hvert emne som en blanding af ord. 

- Antagelse om at en tekst er udgjort af $K$ topics (modsat bin√¶re klassificeringer af tekster).

    - En tekst kan v√¶re $70\%$ **topic A**, $10\%$ **topic B**, $20\%$ **topic C**

- Bygger p√• en ‚Äùbag-of-words‚Äù-tilgang; antagelsen er at ordenes r√¶kkef√∏lge ikke er af betydning [?!]

- Anvendes prim√¶rt p√• en specifik tekstsamling, hvor den estimerede model i har begr√¶nset anvendelighed i andre tekstsamling: alts√•, **lav ekstern validitet**.

:::

> Spg. til teksten?

:::

## Latent Dirichlet allocation (2)

::: {.incremental}

- I mods√¶tning til traditionel klyngeanalyse, er ideen i topic modeling at hvert dokument best√•r af flere forskellige emner i forskellige proportioner, fremfor at hvert dokument typisk kun tilh√∏rer √©n klynge. 

- Et dokument kan derfor tilh√∏re flere emner p√• √©n gang, med forskellig styrke eller sandsynlighed.

- En form for specialiseret klyngeanalyse. 

::: 

## Latent Dirichlet allocation (3)

::: {.columns} 

::: {.column width="70%"}

![](Picture 2.png)

:::

::: {.column width="30%"}

::: {style="font-size: 65%;"}

- `Ord`, $w$: De unikke ord, der fremg√•r i vores samlede tekstmateriale (*vocabular*). 

- `Documenter`, $\textbf{w}$: En sekvens af $N$ ord, $\{ w_{1}, \dots, w_{N} \}$. 

- `Corpus`, $M$: En samling dokumenter, $\{ \textbf{w}_{1}, \dots, \textbf{w}_{N} \}$. 

- `Topics`, $z_{1:K}$: Hvert topic er udtrykt som en sandsynlighedsfordeling af ord, $w$.

- `Topic fordeling`, $\phi_{w,k}$: Andelen af topic $k$ i dokument $\textbf{w}$.

- `emne-tildelingen for ord`, $\theta_{\textbf{w},w}$: Tildeling af topic $k$ til ord $w$ i tekst $\textbf{w}$.

- `Bag-of-Word`: Vektoriceret (simplificeret) repr√¶sentation af en tekst som en opt√¶lling af ord, der fremkommer. Ignorerer grammatik, kontekst, og r√¶kkef√∏lge af ord. 

:::

:::

:::

## Latent Dirichlet allocation (4)

Intuitionen er at hvert dokument er "skabt" p√• baggrund af fordelingen af topics i teksten. Vi vil udlede den sandsynlighedsmodel, der (teoretisk) kunne have "skabt teksten".

::: {.incremental}

- Hvert topic er repr√¶senteret som sandsynligheden for at hvert ord er del af det p√•g√¶ldende topic. 

    > **Hvis en tekst** (*repr√¶senteret som BoW*) **er $80\%$ "politik", $15\%$ "krig" og $5\%$ "forskning", ville en korrekt specificeret model (teoretisk) kunne genskabe denne BoW, ved at v√¶lge $80\%$ "politik-ord", $15\%$ "krigs-ord" og $5\%$ "forsknings-ord", hvis vi gender de underliggende fordelinger af topics of ord** 

- Teknisk vil vi fra vores corpus udlede de skjulte fordelinger: 

    1. `Topic-word` fordelingen: *hvor sandsynligt det er for specifikke ord at optr√¶de i hvert topic*

    2. `Document-topic` fordelingen: *hvor fremtr√¶den hvert topic er i hvert dokument*.

:::

## Latent Dirichlet allocation (5)

> N√•r jeg siger at hvert *topic* i LDA er en "fordeling", hvad tror I jeg mener med det?

::: {.incremental}

- Hvert topic er "blot" en samling af ord, hvor hvert ord har en estimeret sandsynlighed, der indikerer hvor sandsynlige deres fremkomst er i en tekst med et givent topic. 

- Topics kan ogs√• t√¶nkes i "bag"-analogien: hvert tema er en "bag" af ord, hvor nogle er meget fremkomne i √©n "bag", men mindre sandsynlige i en anden "bag". 

    - "Klima" er meget sandsynlig i vores *"milj√∏"*-topic, men fremkommer ogs√• i vores *"skole"*-topic, endog fremkomsten af ordet her er mindre sandsynligt en det f√∏rste topic. 

:::

## Latent Dirichlet allocation (5)

> **Artikel**: Regeringen med minister Y i spidsen pr√¶senterer nye m√•ls√¶tninger for reducering af klimap√•virkning fra landbruget. 

1. Topic 1: *Politik* ($60\%$): regering, m√•ls√¶tning*, minister, reducering, landbrug, $\dots$

2. Topic 2: *Klima* ($40\%$): klima*, landbrug, reducering, $\dots$ 

::: {style="font-size: 65%;"}

| Ord            | Sandsynlighed (Hvor ofte?) | Forklaring                                   |
|----------------|-----------------------------|---------------------------------------------|
| Klima          | H√∏j                         | Ofte centralt i diskussioner                |
| Forurening     | H√∏j                         | Hyppigt omtalt milj√∏problem                 |
| Genbrug        | Middel-h√∏j                  | Almindeligt, men mindre centralt end "klima" |
| B√¶redygtighed  | Middel                      | Vigtigt, men mindre hyppigt                 |
| Plastik        | Middel                      | Ofte relateret til affald og forurening     |
| Vedvarende     | Middel-lav                  | Forekommer i energirelaterede diskussioner  |
| √òkosystem      | Middel-lav                  | Vigtigt, men specialiseret                  |
| Udledninger    | Lav                         | Teknisk, specifikke sammenh√¶nge             |
| Solenergi      | Lav                         | Mere specialiseret del af milj√∏omr√•det      |

: Hyppighed af ord i Klima-topic {tbl-cap}

:::

## Inferens: hvordan "l√¶rer" LDA (1)

::: {.columns} 

::: {.column width="70%"}

![](Picture 2.png)

:::

::: {.column width="30%"}

::: {style="font-size: 65%;"}

::: {.incremental}

- Vi vil udlede den sandsynlighedsmodel, der (teoretisk) kunne have "skabt teksten".

- Konceptuelt, og i praksis, vil vi "reverse-engineer" denne generative proces, der kunne have skabt teksten, ved at estimere de skjulte fordelinger i data: 

    1. `Topic-word` fordelingen, $\phi$: *hvor sandsynligt det er for specifikke ord at optr√¶de i hvert topic*

    2. `Document-topic` fordelingen, $\theta$: *hvor fremtr√¶den hvert topic er i hvert dokument*.

    3. Vi bruger `Gibbs sampling` til at estimere disse fordelinger. 

:::

:::

:::

:::

## Inferens: hvordan "l√¶rer" LDA (2)

LDA er en `iterativ algoritme`, der tilskriver topics ved:  

::: {.incremental}

::: {style="font-size: 90%;"}

1. Hvor almindeligt/fremtr√¶dende er et topic, $k$, i et givent dokument, $\textbf{w}$ (`Document-topic proportion`)

2. Hvor sandsynligt er det at et ord, $w$, er en del at topic, $k$, p√• tv√¶rs af hele corpusset $M$ (`Topic-word likelihood`)

- Algoritmen giver to outouts: 

    1. `Topic-word` fordelingen, $\phi$: som dermed viser hvilket ord, $w$, definere hvert topic, $k$. 

    2. `Document-topic` fordelingen, $\theta$: som viser sammens√¶tningerne af topics, $K$, for hvert dokument, $\textbf{w}$.

- Vi har nu estimeret en model, der kan: 

    1. Klassificere/tilskrive topics til en tekst. 

    2. Afd√¶kke de underl√¶ggende topics i en samling dokumenter. 

    3. Lave semantisk/tematisk "clustering" af dokumenter, der muligg√∏r flere af anvendelserne, pr√¶senteret i starten. 

:::

:::

## LDA algoritmen (1)

::: {style="font-size: 75%;"}

LDA er iterativ, idet det er en proces, hvor algoritmen laver en gradvis "forbedring" af resultaterne (fordelingerne)---g√∏rer tilpasningsdelen igen og igen---indtil den finder en stabil l√∏sning (konvergerer), hvor hver gentagelse kun √¶ndrer resultaterne minimalt. Ved dette punkt har algoritmen har fundet et stabilt s√¶t af topics. 

```

~~Stiliseret iterativ algoritme~~ 
Input: Et s√¶t af dokumenter D, antal √∏nskede topics K
Output: Document-topic-fordeling og Topic-word-fordeling

Start:
    Initialis√©r tilf√¶ldigt et topic for hvert ord i alle dokumenter

    Gentag
        For hvert dokument W i M:
            For hvert ord w i dokument W:
                Fjern det nuv√¶rende topic k for ordet w

                For hvert topic k fra 1 til K:
                    Beregn sandsynlighed P(k) baseret p√•:
                        a. Hvor hyppigt topic k optr√¶der i dokument W (Document-topic-fordeling)
                        b. Hvor sandsynligt ordet w er under topic k (Topic-word-fordeling)

                V√¶lg nyt topic k' til ordet w baseret p√• sandsynlighedsfordelingen P(k)

                Tildel det nye topic k' til ordet w

        Opdater Document-topic-fordeling og Topic-word-fordeling baseret p√• nye topic-tildelinger

    Indtil (konvergens n√•s: √¶ndringerne mellem iterationer er minimale)

Return√©r Document-topic-fordeling og Topic-word-fordeling
Slut

```

:::

## LDA algoritmen (2)

Vi bruger en afart af `Bayes Theorem` til LDA, hvor vi opdaterer modellens parametre ($\theta, \phi, z$), ved at observere data, ordene, $w$:

$$
P(\theta, \phi, z \mid w, \alpha, \beta) = \frac{P(w \mid z, \phi)P(z \mid \theta)P(\theta \mid \alpha)P(\phi \mid \beta)}{P(w \mid \alpha, \beta)}
$$

Hvor, 

- $w$ er observerede ord.

- $z$ er latente topics.

- $\theta$ er `Document-topic` fordelingen.

- $\phi$ er `Topic-word` fordelingen. 

- $\alpha, \beta$ er `hyperparametre`. 

## LDA algoritmen (3.1)

Vi bruger en afart af `Bayes Theorem` til LDA, hvor vi opdaterer modellens parametre ($\theta, \phi, z$), ved at observere data, ordene, $w$:

$$
P(\theta, \phi, z \mid w, \alpha, \beta) = \frac{P(w \mid z, \phi)P(z \mid \theta)P(\theta \mid \alpha)P(\phi \mid \beta)}{P(w \mid \alpha, \beta)}
$$

::: {.incremental}

- $P(w|z, \phi)$ er sandsynligheden for at observere ordene $w$, hvis vi kender topic-tildelingerne $z$ og topic-word fordelingerne $\phi$.

    - <p><span style="font-size:0.7em">*N√•r vi ved, hvilke topics ord kommer fra, og hvordan topics "genererer" ord, kan vi beregne sandsynligheden for ordene i dokumenterne.*</span></p>

- $P(z|\theta)$ er sandsynligheden for at hvert ord f√•r en bestemt topic-tildeling $z$, givet dokumenternes emnefordeling $\theta$.

    - <p><span style="font-size:0.7em">*Hvis vi ved, hvor meget hvert topic fylder i dokumentet, kan vi beregne sandsynligheden for, at ordet tilh√∏rer et bestemt topic.*</span></p>

:::

## LDA algoritmen (3.2)

Vi bruger en afart af `Bayes Theorem` til LDA, hvor vi opdaterer modellens parametre ($\theta, \phi, z$), ved at observere data, ordene, $w$:

$$
P(\theta, \phi, z \mid w, \alpha, \beta) = \frac{P(w \mid z, \phi)P(z \mid \theta)P(\theta \mid \alpha)P(\phi \mid \beta)}{P(w \mid \alpha, \beta)}
$$

::: {.incremental}

- $P(\theta|\alpha)$ er sandsynligheden for document-topic fordelingen $\theta$, givet *Dirichlet-fordelingen* med hyperparametre $\alpha$.

    - <p><span style="font-size:0.7em">*Vi starter med en antagelse om, hvordan topics typisk fordeler sig i dokumenter (bestemt af \alpha). Denne antagelse justeres iterativt, n√•r vi ser p√• data.*</span></p>

- $P(\psi|\beta)$ er sandsynligheden for topic-word fordelingen $\psi$, givet *Dirichlet-fordelingen* med hyperparametre $\beta$.

    - <p><span style="font-size:0.7em">*Vi starter med en antagelse om, hvordan ordene typisk fordeler sig i topics (bestemt af $\beta$). Denne antagelse justeres iterativt ud fra data.*</span></p>

:::

## LDA algoritmen (3.3)

Vi bruger en afart af `Bayes Theorem` til LDA, hvor vi opdaterer modellens parametre ($\theta, \phi, z$), ved at observere data, ordene, $w$:

$$
P(\theta, \phi, z \mid w, \alpha, \beta) = \frac{P(w \mid z, \phi)P(z \mid \theta)P(\theta \mid \alpha)P(\phi \mid \beta)}{P(w \mid \alpha, \beta)}
$$

::: {.incremental}

- $P(w|\alpha, \beta)$ er en *normaliseringsfaktor*, der sikrer at sandsynlighederne summerer til 1.
    
:::

## LDA algoritmen (3.4)

Vi bruger en afart af `Bayes Theorem` til LDA, hvor vi opdaterer modellens parametre ($\theta, \phi, z$), ved at observere data, ordene, $w$:

$$
P(\theta, \phi, z \mid w, \alpha, \beta) = \frac{P(w \mid z, \phi)P(z \mid \theta)P(\theta \mid \alpha)P(\phi \mid \beta)}{P(w \mid \alpha, \beta)}
$$

Dette sker ved inferens, hvor algoritmen ser p√• data (tekster) og finder skjulte m√∏nstre i ordforekomster.

Formlen beskriver, hvordan LDA bruger data (ordene i dokumenterne) og tidligere antagelser (hyperparametrene) til iterativt at finde og opdatere emner og deres fordelinger.

Den g√∏r dette iterativt, indtil sandsynligheden for at observere vores data (givet modellerede emner) bliver maksimal‚Äîog resultatet bliver en realistisk, meningsfuld inddeling af store tekstsamlinger i skjulte emner.

Grundl√¶ggende vil vi finde ...  der fors√∏ger at finde de v√¶rdier for Œ∏Œ∏, œïœï og zz, der g√∏r, at sandsynligheden bliver st√∏rst muligt (maksimerer posterior-sandsynligheden).

Hypotetisk output: 

Dokument-emne fordeling (Œ∏Œ∏):

    √ân artikel best√•r m√•ske af 70% politik, 20% √∏konomi og 10% teknologi.

Emne-ord fordeling (œïœï):

    Emnet "politik" kunne v√¶re kendetegnet ved ord som "regering", "valg", "minister".
    Emnet "√∏konomi" ved ord som "inflation", "arbejdsl√∏shed", "v√¶kst".

LDA starter med tilf√¶ldige g√¶t om disse fordelinger. Derefter opdaterer den iterativt:

    Hver gang algoritmen g√•r igennem data, f√•r den bedre "g√¶t".
    Dokumenterne stabiliserer sig i klare emne-proportioner, og emnerne stabiliserer sig med klare ord-proportioner.

Til sidst f√•r vi:

    Klare emner med meningsfulde ord (emne-ord fordelinger).
    Tydelige proportioner af emner i dokumenterne (dokument-emne fordelinger).

## LDA algoritmen (3.5)

Forestil dig dette simple eksempel:

    Tekst:
    ‚ÄùRegeringen vedtog ny lovgivning efter valget.‚Äù

Antag, at vi har to emner:

    Emne A (politik): ‚Äùregering‚Äù, ‚Äùvalg‚Äù, ‚Äùlovgivning‚Äù
    Emne B (sport): ‚Äùkamp‚Äù, ‚Äùm√•l‚Äù, ‚Äùsejr‚Äù

Initialisering:

    Vi tildeler tilf√¶ldigt emner til ord, fx:
        ‚Äùregeringen‚Äù: emne B
        ‚Äùvedtog‚Äù: emne A
        ‚Äùny‚Äù: emne B
        ‚Äùlovgivning‚Äù: emne A
        ‚Äùefter‚Äù: emne B
        ‚Äùvalget‚Äù: emne B

Iteration:

Vi tager √©t ord ad gangen, fjerner dets aktuelle emne og sp√∏rger:

    Hvor sandsynligt er ordet under hvert emne?
        ‚Äùregeringen‚Äù optr√¶der mest med ‚Äùvalg‚Äù, ‚Äùlovgivning‚Äù osv. ‚Üí politik sandsynligt.
        ‚Äùm√•l‚Äù og ‚Äùsejr‚Äù ses ikke n√¶r ‚Äùregeringen‚Äù ‚Üí sport mindre sandsynligt.

    Hvor sandsynligt er hvert emne i dette dokument?
        Flest ord handler allerede om politik ‚Üí emne A (politik) sandsynligt.

Efter flere iterationer flytter ordene sig langsomt over i mere passende emner. Sandsynlighederne bliver mere pr√¶cise og stabile.
Efter mange iterationer f√•r vi:

    ‚Äùregeringen‚Äù ‚Üí Emne A (politik) med h√∏j sandsynlighed.
    ‚Äùlovgivning‚Äù ‚Üí Emne A (politik)
    ‚Äùvalget‚Äù ‚Üí Emne A (politik)
    ‚Äùm√•l‚Äù, ‚Äùsejr‚Äù ‚Üí Emne B (sport) (hvis teksten indeholdt dem)

üéØ Konvergens og Resultater

N√•r algoritmen gentager denne proces mange gange, opst√•r:

    Stabile emne-ord fordelinger:
    Vi kan se tydelige emner som fx ‚Äùpolitik‚Äù, ‚Äùsport‚Äù, ‚Äù√∏konomi‚Äù, osv.

    Klare dokument-emne fordelinger:
    Dokumenter f√•r tydelige proportioner af hvert emne.


## L`D`A algoritmen (3.6)

I LDA bruges Dirichlet-fordelingen som prior, dvs. en startantagelse om, hvordan fordelingerne b√∏r se ud:

    Iteration efter iteration opdaterer algoritmen fordelingerne af emner i dokumenter (Œ∏Œ∏) og fordelinger af ord i emner (œïœï).
    Til sidst stabiliserer disse fordelinger sig omkring v√¶rdier, der passer bedst med data.

S√• Dirichlet-fordelingen hj√¶lper algoritmen med at finde realistiske og stabile fordelinger af ord og emner.

Dirichlet-fordelingen bruges i LDA, fordi den er specielt designet til fordelinger over proportioner (der summerer til 1).
Œ±-parametrene bestemmer graden af sparsommelighed eller ligelighed i fordelingerne.
I praksis fungerer Dirichlet-fordelingen som en startantagelse, der opdateres iterativt i LDA-processen, hvilket f√∏rer til mere realistiske og meningsfulde emner.

Hvorfor Dirichlet-fordelingen bruges i LDA (Intuition)

I LDA modellerer vi sandsynligheder eller proportioner, fx:

    Dokumenter indeholder forskellige emner med forskellige proportioner.
    Emner indeholder forskellige ord med forskellige proportioner.

Disse proportioner skal opfylde f√∏lgende:

    Alle proportioner er positive eller nul.
    Alle proportioner skal summere til 1 (fx 40% politik + 30% sport + 30% √∏konomi = 100%).

Dirichlet-fordelingen er netop designet til at modellere denne slags proportioner, fordi den definerer en sandsynlighedsfordeling over proportioner, der summerer til √©n.

Dirichlet-fordelingen er en sandsynlighedsfordeling over vektorer af proportioner. Matematikken:

En tilf√¶ldig vektor Œ∏=(Œ∏1,Œ∏2,‚Ä¶,Œ∏K) f√∏lger en Dirichlet-fordeling med parametre Œ±=(Œ±1,Œ±2,‚Ä¶,Œ±K), hvis sandsynlighedst√¶thedsfunktionen (PDF) er givet ved:

$$
f(\theta; \alpha) = \frac{\Gamma\left(\sum_{i=1}^{K}\alpha_i\right)}{\prod_{i=1}^{K}\Gamma(\alpha_i)} \prod_{i=1}^{K}\theta_i^{\alpha_i - 1}
$$

Hver parameter Œ±iŒ±i‚Äã kan t√¶nkes som en "prior count" eller ‚Äùforventet v√¶gt‚Äù p√• kategori/emne ii:

    Lav Œ± (fx Œ± < 1):
        Sandsynlighedsfordelingen koncentreres omkring f√• emner.
        Resultatet: √©t eller f√• emner dominerer (sparsom fordeling).

    Œ± omkring 1 (fx Œ± = 1):
        Emnerne er omtrent ens sandsynlige.
        Resultatet: Mere tilf√¶ldige og j√¶vne emnefordelinger.

    Œ± h√∏jere end 1 (fx Œ± > 1):
        Mere j√¶vnt fordelt p√• tv√¶rs af alle emner.
        Resultatet: Dokumenter har ofte en mere ligelig blanding af mange emner.

Kort sagt, Œ± styrer alts√•, hvor meget variation og sparsommelighed der er i fordelingerne.

LDA bruger Dirichlet-fordelingen to steder:

    Dokument-emne-fordelingen (Œ∏Œ∏):
    Hvert dokument dd v√¶lger en fordeling over emner fra en Dirichlet-fordeling:
    Œ∏d‚Äã‚àºDirichlet(Œ±)

    Emne-ord-fordelingen:
    Hvert emne v√¶lger en fordeling over ord fra en Dirichlet-fordeling med parameter Œ≤Œ≤:
    œïk‚Äã‚àºDirichlet(Œ≤)

I praksis betyder dette, at LDA starter med:

    At g√¶tte en tilf√¶ldig blanding af emner i hvert dokument (styret af Œ±Œ±).
    At v√¶lge en tilf√¶ldig sammens√¶tning af ord inden for hvert emne (styret af Œ≤Œ≤).



Forestil dig, at vi har 3 emner (fx Politik, Sport, √òkonomi):

Lad os v√¶lge forskellige Œ±-parametre:
Eksempel A (Œ± = [1, 1, 1])

    Emnerne er lige sandsynlige.
    Sandsynlige blandinger:
    (0.33, 0.33, 0.33), (0.4, 0.3, 0.3), (0.3, 0.3, 0.4) osv.

Eksempel B (Œ± = [0.1, 0.1, 0.1])

    Meget sparsomme fordelinger.
    Resultatet vil typisk v√¶re: [0.95, 0.04, 0.01] ‚Äì √©t emne dominerer helt.

Eksempel C (Œ± = [10, 10, 10])

    Meget j√¶vne fordelinger: [0.34, 0.33, 0.33].
    Ingen emner er klart dominerende, vi f√•r en j√¶vn fordeling.

```{python}
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import dirichlet

# Œ±-parametre (pr√∏v at √¶ndre disse!)
alpha = [1, 1, 1]

# Generer data fra Dirichlet-fordelingen
samples = dirichlet.rvs(alpha, size=5000)

# Visualisering
fig = plt.figure(figsize=(7, 6))
ax = fig = plt.figure().add_subplot(projection='3d')
ax = plt.axes(projection='3d')

ax.scatter(samples[:,0], samples[:,1], samples[:,2], c='green', marker='o', alpha=0.3)
ax.set_xlabel('Emne 1')
ax.set_ylabel('Emne 2')
ax.set_zlabel('Emne 3')

plt.title('Dirichlet-fordeling med Œ± = [1, 1, 1]')
plt.show()

```

## Fordele og ulemper: opsummering 



Strengths of LDA
Clear Probabilistic Interpretation:

    LDA's explicit probabilistic formulation makes it interpretable and easy to understand mathematically.

Simple Implementation and Efficiency:

    Fast, mature implementations (e.g., Gensim, MALLET, Scikit-learn) are readily available.
    Scales well to large datasets with Gibbs sampling or variational inference.

Robustness and Flexibility:

    Effective even with relatively small datasets.
    Clear mechanisms (via priors Œ±Œ± and Œ≤Œ≤) to control topic sparsity.

Human-readable Outputs:

    Topics represented as easily interpretable distributions over words.

Weaknesses of LDA
Bag-of-Words Limitation:

    LDA disregards word order and semantics, relying solely on word frequencies and co-occurrences.
    Cannot distinguish nuanced semantic differences or polysemy (words with multiple meanings).

Sensitive to Hyperparameters and Number of Topics:

    Choosing the number of topics KK and hyperparameters Œ±Œ± and Œ≤Œ≤ often involves trial-and-error or heuristics.

Difficulties Handling Short Texts:

    Documents with fewer words (e.g., tweets) offer less statistical evidence, resulting in poorer topic quality.

Context Insensitivity:

    Unable to leverage linguistic or semantic context. For example, "apple" (fruit vs. company) is ambiguous in LDA.


# LDA i praksis