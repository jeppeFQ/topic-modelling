---
title: "Topic Modeling"
encoding: "UTF-8"
execute:
  echo: false
format:
  revealjs:
    slide-number: c
    show-slide-number: print
    embed-resources: true
    self-contained-math: true
    smaller: true
    scrollable: true
    theme: dark 
    html-math-method:
      method: mathjax
      url: "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML"
    footer: "Jeppe F. Qvist | 18. marts 2025"
---

## Dagens program 

1. Introduktion til Topic Modeling med særligt fokus på LDA. 

2. LDA i praksis.

*Til denne forelæsning introduceres topic modeling. Topic models er usuperviseret maskinlæringsmodeller, der har til formål at udlede temaer af tekster baseret på ord, der går igen på tværs af tekster. Der findes forskellige algoritmer til og varianter af topic modeling, men til denne undervisningsgang fokuseres på den mest almindelige algoritme/model kaldet Latent Dirichlet allocation (LDA). Det bliver både gennemgået, hvordan modellen fungerer samt hvordan man anvender disse i Python med pakken gensim.*

::: {style="font-size: 75%;"}

- Asmussen, C. B., Møller, C. (2019). Smart literature review: A practical topic modelling approach to exploratory literature review. *Journal of Big Data*, 6(1), 1-18. 

- Blei, D. M., Ng, A. Y., Jordan, M. (2003). Latent Dirichlet Allocation. *Journal of Machine Learning Research*, 3, 993–1022. 

- Blei, D. M. (2012). Probabilistic topic models. *Communications of the ACM*, 55(4), 77–84. 

- Reed, C. (2012). *Latent Dirichlet Allocation: Towards a Deeper Understanding.* 

:::

## Topic Modeling

> *Latent Dirichlet allocation*

## *Topic* Modeling

> **Latent** *Dirichlet allocation*

## Topic Modeling: *anvendelsesmuligheder*

Topic modellering er en ML metode, der kan afdække `skjulte` topics eller tematikker i en stor samling tekst dokumenter. 

- **Dokument-"klynger"**: automatisk gruppering af tekster i kategorier. 

- **Indholdsanbefaling**: find (og grupper) tekster, der ligner hinanden med henblik på abefaling. "Vi tror du vil kunne lide ... "

- **Informationssøgning**: Forbedring af søgninger, vil også at inkludere ligende tekster. 

- **Opsummeringer**: Lav overblik over hvilke topics er de mest fremkomne. 

- **Monoterering**: Automatiske overblik over trends (fx på SoMe).

## Topic Modeling: *generelle begrænsninger*

- **Tolkning**: Topics er typiske abstrakte og kræver en kvalitativ (menneskelig) evaluering. 

- **Parameter-sensitivitet**: Modeller kan være sensitive overfor hvordan vi specificere modellen (fx antallet af topics, som vi selv skal angive).

- **Bag-of-Words antagelse**: Traditionelle topic modeller arbejder med BoW representation af tekst (mere herom snart). I næste forelæsning lærer Kristian jer om, hvordan i "overkommer" dette med nyere tilgange. 

## Topic Modeling

> ***Usuperviseret makinlæringsmodel***

## Topic modeling (1)

> ***Usuperviseret makinlæringsmodel***

> Hvad er målet i usurperviseret ML? 

## Topic modeling (2)

> ***Usuperviseret makinlæringsmodel***

> Hvad er målet i usurperviseret ML? 

::: {style="font-size: 75%;"}
::: {.incremental}

Pointer fra Rolfs anden forelæsning (***Hvad er machine learning?***): 

1. I usuperviseret ML er målet helt enkelt: *kompleksitetsreduktion*

2. Vi har ikke nødvendigvis et klart mål til vores algoritme, vi har i stedet et ønske om, at forstå data bedre.

3. Vi har spørgsmål som: (1) Hvordan er de ”rå” grupper i data? (2) Hvad ”stikker ud” når jeg viser min computer de her grupper af data? (3) Hvilken ”sekvens” gør mennesker, når de bliver sat i en bestemt situation?

- Usuperviserede modeller er ofte mere ”grove” [end superviserede modeller]---vi kan ikke stille et specifikt spørgsmål til dem og deres måde at løse problemet på er, ligesom ved klassisk statistik, ved at ”æde” det data vi giver og så afleverer de et svar til os.

:::
:::

## Topic modeling (3)

> Hvad er *kompleksitetsreduktion*, og hvad er formålet med det? 

## Topic modeling (4)

> Hvad er *kompleksitetsreduktion*, og hvad er formålet med det? 

::: {style="font-size: 75%;"}
::: {.incremental}

Pointer fra Rolfs tredje forelæsning (***Avanceret klyngeanalyse og datareduktion)***: 

- I dag er konteksten `tekst` (ikke mennesker) og `iterative algoritmer` (probalistiske modeller; ikke rekursive algoritmer), men nogle grundlæggende ideer og principper går igen:

1. Dimensioner i data er bagvedliggende faktorer, der forklarer ”ting” i data vi ellers ikke havde opdaget eller ting vi havde opdaget men fordi vi har så meget data vil tage os for evigt at forholde os til. 

2. Formålet (...) er, at vi gerne vil reducere hvor ”vildt” det hele er for os. Vi kan, teknisk set, gøre det på alle former for data, men der er forskellige måder vi kan håndtere forskellige typer af data på. [Fx:]

    - Klyngedannelse, i sin absolut simpleste form, er bare det---at skabe klynger af ”ting” så de giver mening som mere end entiteter. 

    - I sociologien [og politiologien] er det stadig at finde ”dem der”, men computeren gør det muligt at finde ”dem der” uden at have en forudindtaget ide om, hvem ”de” er. 

3. Når man taler om afstand i klyngeanalyser er det ikke en fysisk afstand, men nærmere en afstand mellem holdninger, følelser mm. [Vi kan også tale om afstande mellem tekster]

:::
:::

## Latent Dirichlet allocation (1)

LDA er en probabilistisk generativ model, der blev introduceret af `Blei, Ng, and Jordan (2003)`. Metoden modellere et dokuments latente topics, hvor hvert topic er repræsenteret af en ***fordeling over ord***. 

::: {style="font-size: 75%;"}
::: {.incremental}

- Giver overblik over et større (*tekst*)materiale ved inddeling og kategorisering; *”utility [rather than] accuracy”* (Asmussen og Møller, 2019). 

- Målet er at identificere skjulte emner i en samling af dokumenter ved at analysere ordforekomster og deres sammenhænge.

    - Intuitionen er at hvert dokument repræsenteres som en blanding af emner, og hvert emne som en blanding af ord. 

- Antagelse om at en tekst er udgjort af $K$ topics (modsat binære klassificeringer af tekster).

    - En tekst kan være $70\%$ **topic A**, $10\%$ **topic B**, $20\%$ **topic C**

- Bygger på en ”bag-of-words”-tilgang; antagelsen er at ordenes rækkefølge ikke er af betydning [?!]

- Anvendes primært på en specifik tekstsamling, hvor den estimerede model i har begrænset anvendelighed i andre tekstsamling: altså, **lav ekstern validitet**.

:::

> Spg. til teksten?

:::

## Latent Dirichlet allocation (2)

::: {.incremental}

- I modsætning til traditionel klyngeanalyse, er ideen i topic modeling at hvert dokument består af flere forskellige emner i forskellige proportioner, fremfor at hvert dokument typisk kun tilhører én klynge. 

- Et dokument kan derfor tilhøre flere emner på én gang, med forskellig styrke eller sandsynlighed.

- En form for specialiseret klyngeanalyse. 

::: 

## Latent Dirichlet allocation (3)

::: {.columns} 

::: {.column width="70%"}

![](Picture 2.png)

:::

::: {.column width="30%"}

::: {style="font-size: 65%;"}

- `Ord`, $w$: De unikke ord, der fremgår i vores samlede tekstmateriale (*vocabular*). 

- `Documenter`, $\textbf{w}$: En sekvens af $N$ ord, $\{ w_{1}, \dots, w_{N} \}$. 

- `Corpus`, $M$: En samling dokumenter, $\{ \textbf{w}_{1}, \dots, \textbf{w}_{N} \}$. 

- `Topics`, $z_{1:K}$: Hvert topic er udtrykt som en sandsynlighedsfordeling af ord, $w$.

- `Topic fordeling`, $\phi_{w,k}$: Andelen af topic $k$ i dokument $\textbf{w}$.

- `emne-tildelingen for ord`, $\theta_{\textbf{w},w}$: Tildeling af topic $k$ til ord $w$ i tekst $\textbf{w}$.

- `Bag-of-Word`: Vektoriceret (simplificeret) repræsentation af en tekst som en optælling af ord, der fremkommer. Ignorerer grammatik, kontekst, og rækkefølge af ord. 

:::

:::

:::

## Latent Dirichlet allocation (4)

Intuitionen er at hvert dokument er "skabt" på baggrund af fordelingen af topics i teksten. Vi vil udlede den sandsynlighedsmodel, der (teoretisk) kunne have "skabt teksten".

::: {.incremental}

- Hvert topic er repræsenteret som sandsynligheden for at hvert ord er del af det pågældende topic. 

    > **Hvis en tekst** (*repræsenteret som BoW*) **er $80\%$ "politik", $15\%$ "krig" og $5\%$ "forskning", ville en korrekt specificeret model (teoretisk) kunne genskabe denne BoW, ved at vælge $80\%$ "politik-ord", $15\%$ "krigs-ord" og $5\%$ "forsknings-ord", hvis vi gender de underliggende fordelinger af topics of ord** 

- Teknisk vil vi fra vores corpus udlede de skjulte fordelinger: 

    1. `Topic-word` fordelingen: *hvor sandsynligt det er for specifikke ord at optræde i hvert topic*

    2. `Document-topic` fordelingen: *hvor fremtræden hvert topic er i hvert dokument*.

:::

## Latent Dirichlet allocation (5)

> Når jeg siger at hvert *topic* i LDA er en "fordeling", hvad tror I jeg mener med det?

::: {.incremental}

- Hvert topic er "blot" en samling af ord, hvor hvert ord har en estimeret sandsynlighed, der indikerer hvor sandsynlige deres fremkomst er i en tekst med et givent topic. 

- Topics kan også tænkes i "bag"-analogien: hvert tema er en "bag" af ord, hvor nogle er meget fremkomne i én "bag", men mindre sandsynlige i en anden "bag". 

    - "Klima" er meget sandsynlig i vores *"miljø"*-topic, men fremkommer også i vores *"skole"*-topic, endog fremkomsten af ordet her er mindre sandsynligt en det første topic. 

:::

## Latent Dirichlet allocation (5)

> **Artikel**: Regeringen med minister Y i spidsen præsenterer nye målsætninger for reducering af klimapåvirkning fra landbruget. 

1. Topic 1: *Politik* ($60\%$): regering, målsætning*, minister, reducering, landbrug, $\dots$

2. Topic 2: *Klima* ($40\%$): klima*, landbrug, reducering, $\dots$ 

::: {style="font-size: 65%;"}

| Ord            | Sandsynlighed (Hvor ofte?) | Forklaring                                   |
|----------------|-----------------------------|---------------------------------------------|
| Klima          | Høj                         | Ofte centralt i diskussioner                |
| Forurening     | Høj                         | Hyppigt omtalt miljøproblem                 |
| Genbrug        | Middel-høj                  | Almindeligt, men mindre centralt end "klima" |
| Bæredygtighed  | Middel                      | Vigtigt, men mindre hyppigt                 |
| Plastik        | Middel                      | Ofte relateret til affald og forurening     |
| Vedvarende     | Middel-lav                  | Forekommer i energirelaterede diskussioner  |
| Økosystem      | Middel-lav                  | Vigtigt, men specialiseret                  |
| Udledninger    | Lav                         | Teknisk, specifikke sammenhænge             |
| Solenergi      | Lav                         | Mere specialiseret del af miljøområdet      |

: Hyppighed af ord i Klima-topic {tbl-cap}

:::

## Inferens: hvordan "lærer" LDA (1)

::: {.columns} 

::: {.column width="70%"}

![](Picture 2.png)

:::

::: {.column width="30%"}

::: {style="font-size: 65%;"}

::: {.incremental}

- Vi vil udlede den sandsynlighedsmodel, der (teoretisk) kunne have "skabt teksten".

- Konceptuelt, og i praksis, vil vi "reverse-engineer" denne generative proces, der kunne have skabt teksten, ved at estimere de skjulte fordelinger i data: 

    1. `Topic-word` fordelingen, $\phi$: *hvor sandsynligt det er for specifikke ord at optræde i hvert topic*

    2. `Document-topic` fordelingen, $\theta$: *hvor fremtræden hvert topic er i hvert dokument*.

    3. Vi bruger `Gibbs sampling` til at estimere disse fordelinger. 

:::

:::

:::

:::

## Inferens: hvordan "lærer" LDA (2)

LDA er en `iterativ algoritme`, der tilskriver topics ved:  

::: {.incremental}

::: {style="font-size: 90%;"}

1. Hvor almindeligt/fremtrædende er et topic, $k$, i et givent dokument, $\textbf{w}$ (`Document-topic proportion`)

2. Hvor sandsynligt er det at et ord, $w$, er en del at topic, $k$, på tværs af hele corpusset $M$ (`Topic-word likelihood`)

- Algoritmen giver to outouts: 

    1. `Topic-word` fordelingen, $\phi$: som dermed viser hvilket ord, $w$, definere hvert topic, $k$. 

    2. `Document-topic` fordelingen, $\theta$: som viser sammensætningerne af topics, $K$, for hvert dokument, $\textbf{w}$.

- Vi har nu estimeret en model, der kan: 

    1. Klassificere/tilskrive topics til en tekst. 

    2. Afdække de underlæggende topics i en samling dokumenter. 

    3. Lave semantisk/tematisk "clustering" af dokumenter, der muliggør flere af anvendelserne, præsenteret i starten. 

:::

:::

## LDA algoritmen (1)

::: {style="font-size: 75%;"}

LDA er iterativ, idet det er en proces, hvor algoritmen laver en gradvis "forbedring" af resultaterne (fordelingerne)---gører tilpasningsdelen igen og igen---indtil den finder en stabil løsning (konvergerer), hvor hver gentagelse kun ændrer resultaterne minimalt. Ved dette punkt har algoritmen har fundet et stabilt sæt af topics. 

```

~~Stiliseret iterativ algoritme~~ 
Input: Et sæt af dokumenter D, antal ønskede topics K
Output: Document-topic-fordeling og Topic-word-fordeling

Start:
    Initialisér tilfældigt et topic for hvert ord i alle dokumenter

    Gentag
        For hvert dokument W i M:
            For hvert ord w i dokument W:
                Fjern det nuværende topic k for ordet w

                For hvert topic k fra 1 til K:
                    Beregn sandsynlighed P(k) baseret på:
                        a. Hvor hyppigt topic k optræder i dokument W (Document-topic-fordeling)
                        b. Hvor sandsynligt ordet w er under topic k (Topic-word-fordeling)

                Vælg nyt topic k' til ordet w baseret på sandsynlighedsfordelingen P(k)

                Tildel det nye topic k' til ordet w

        Opdater Document-topic-fordeling og Topic-word-fordeling baseret på nye topic-tildelinger

    Indtil (konvergens nås: ændringerne mellem iterationer er minimale)

Returnér Document-topic-fordeling og Topic-word-fordeling
Slut

```

:::

## LDA algoritmen (2)

Vi bruger en afart af `Bayes Theorem` til LDA, hvor vi opdaterer modellens parametre ($\theta, \phi, z$), ved at observere data, ordene, $w$:

$$
P(\theta, \phi, z \mid w, \alpha, \beta) = \frac{P(w \mid z, \phi)P(z \mid \theta)P(\theta \mid \alpha)P(\phi \mid \beta)}{P(w \mid \alpha, \beta)}
$$

Hvor, 

- $w$ er observerede ord.

- $z$ er latente topics.

- $\theta$ er `Document-topic` fordelingen.

- $\phi$ er `Topic-word` fordelingen. 

- $\alpha, \beta$ er `hyperparametre`. 

## LDA algoritmen (3.1)

Vi bruger en afart af `Bayes Theorem` til LDA, hvor vi opdaterer modellens parametre ($\theta, \phi, z$), ved at observere data, ordene, $w$:

$$
P(\theta, \phi, z \mid w, \alpha, \beta) = \frac{P(w \mid z, \phi)P(z \mid \theta)P(\theta \mid \alpha)P(\phi \mid \beta)}{P(w \mid \alpha, \beta)}
$$

::: {.incremental}

- $P(w|z, \phi)$ er sandsynligheden for at observere ordene $w$, hvis vi kender topic-tildelingerne $z$ og topic-word fordelingerne $\phi$.

    - <p><span style="font-size:0.7em">*Når vi ved, hvilke topics ord kommer fra, og hvordan topics "genererer" ord, kan vi beregne sandsynligheden for ordene i dokumenterne.*</span></p>

- $P(z|\theta)$ er sandsynligheden for at hvert ord får en bestemt topic-tildeling $z$, givet dokumenternes emnefordeling $\theta$.

    - <p><span style="font-size:0.7em">*Hvis vi ved, hvor meget hvert topic fylder i dokumentet, kan vi beregne sandsynligheden for, at ordet tilhører et bestemt topic.*</span></p>

:::

## LDA algoritmen (3.2)

Vi bruger en afart af `Bayes Theorem` til LDA, hvor vi opdaterer modellens parametre ($\theta, \phi, z$), ved at observere data, ordene, $w$:

$$
P(\theta, \phi, z \mid w, \alpha, \beta) = \frac{P(w \mid z, \phi)P(z \mid \theta)P(\theta \mid \alpha)P(\phi \mid \beta)}{P(w \mid \alpha, \beta)}
$$

::: {.incremental}

- $P(\theta|\alpha)$ er sandsynligheden for document-topic fordelingen $\theta$, givet *Dirichlet-fordelingen* med hyperparametre $\alpha$.

    - <p><span style="font-size:0.7em">*Vi starter med en antagelse om, hvordan topics typisk fordeler sig i dokumenter (bestemt af \alpha). Denne antagelse justeres iterativt, når vi ser på data.*</span></p>

- $P(\psi|\beta)$ er sandsynligheden for topic-word fordelingen $\psi$, givet *Dirichlet-fordelingen* med hyperparametre $\beta$.

    - <p><span style="font-size:0.7em">*Vi starter med en antagelse om, hvordan ordene typisk fordeler sig i topics (bestemt af $\beta$). Denne antagelse justeres iterativt ud fra data.*</span></p>

:::

## LDA algoritmen (3.3)

Vi bruger en afart af `Bayes Theorem` til LDA, hvor vi opdaterer modellens parametre ($\theta, \phi, z$), ved at observere data, ordene, $w$:

$$
P(\theta, \phi, z \mid w, \alpha, \beta) = \frac{P(w \mid z, \phi)P(z \mid \theta)P(\theta \mid \alpha)P(\phi \mid \beta)}{P(w \mid \alpha, \beta)}
$$

::: {.incremental}

- $P(w|\alpha, \beta)$ er en *normaliseringsfaktor*, der sikrer at sandsynlighederne summerer til 1.
    
:::

## LDA algoritmen (3.4)

Vi bruger en afart af `Bayes Theorem` til LDA, hvor vi opdaterer modellens parametre ($\theta, \phi, z$), ved at observere data, ordene, $w$:

$$
P(\theta, \phi, z \mid w, \alpha, \beta) = \frac{P(w \mid z, \phi)P(z \mid \theta)P(\theta \mid \alpha)P(\phi \mid \beta)}{P(w \mid \alpha, \beta)}
$$

Dette sker ved inferens, hvor algoritmen ser på data (tekster) og finder skjulte mønstre i ordforekomster.

Formlen beskriver, hvordan LDA bruger data (ordene i dokumenterne) og tidligere antagelser (hyperparametrene) til iterativt at finde og opdatere emner og deres fordelinger.

Den gør dette iterativt, indtil sandsynligheden for at observere vores data (givet modellerede emner) bliver maksimal—og resultatet bliver en realistisk, meningsfuld inddeling af store tekstsamlinger i skjulte emner.

Grundlæggende vil vi finde ...  der forsøger at finde de værdier for θθ, ϕϕ og zz, der gør, at sandsynligheden bliver størst muligt (maksimerer posterior-sandsynligheden).

Hypotetisk output: 

Dokument-emne fordeling (θθ):

    Én artikel består måske af 70% politik, 20% økonomi og 10% teknologi.

Emne-ord fordeling (ϕϕ):

    Emnet "politik" kunne være kendetegnet ved ord som "regering", "valg", "minister".
    Emnet "økonomi" ved ord som "inflation", "arbejdsløshed", "vækst".

LDA starter med tilfældige gæt om disse fordelinger. Derefter opdaterer den iterativt:

    Hver gang algoritmen går igennem data, får den bedre "gæt".
    Dokumenterne stabiliserer sig i klare emne-proportioner, og emnerne stabiliserer sig med klare ord-proportioner.

Til sidst får vi:

    Klare emner med meningsfulde ord (emne-ord fordelinger).
    Tydelige proportioner af emner i dokumenterne (dokument-emne fordelinger).

## LDA algoritmen (3.5)

Forestil dig dette simple eksempel:

    Tekst:
    ”Regeringen vedtog ny lovgivning efter valget.”

Antag, at vi har to emner:

    Emne A (politik): ”regering”, ”valg”, ”lovgivning”
    Emne B (sport): ”kamp”, ”mål”, ”sejr”

Initialisering:

    Vi tildeler tilfældigt emner til ord, fx:
        ”regeringen”: emne B
        ”vedtog”: emne A
        ”ny”: emne B
        ”lovgivning”: emne A
        ”efter”: emne B
        ”valget”: emne B

Iteration:

Vi tager ét ord ad gangen, fjerner dets aktuelle emne og spørger:

    Hvor sandsynligt er ordet under hvert emne?
        ”regeringen” optræder mest med ”valg”, ”lovgivning” osv. → politik sandsynligt.
        ”mål” og ”sejr” ses ikke nær ”regeringen” → sport mindre sandsynligt.

    Hvor sandsynligt er hvert emne i dette dokument?
        Flest ord handler allerede om politik → emne A (politik) sandsynligt.

Efter flere iterationer flytter ordene sig langsomt over i mere passende emner. Sandsynlighederne bliver mere præcise og stabile.
Efter mange iterationer får vi:

    ”regeringen” → Emne A (politik) med høj sandsynlighed.
    ”lovgivning” → Emne A (politik)
    ”valget” → Emne A (politik)
    ”mål”, ”sejr” → Emne B (sport) (hvis teksten indeholdt dem)

🎯 Konvergens og Resultater

Når algoritmen gentager denne proces mange gange, opstår:

    Stabile emne-ord fordelinger:
    Vi kan se tydelige emner som fx ”politik”, ”sport”, ”økonomi”, osv.

    Klare dokument-emne fordelinger:
    Dokumenter får tydelige proportioner af hvert emne.


## L`D`A algoritmen (3.6)

I LDA bruges Dirichlet-fordelingen som prior, dvs. en startantagelse om, hvordan fordelingerne bør se ud:

    Iteration efter iteration opdaterer algoritmen fordelingerne af emner i dokumenter (θθ) og fordelinger af ord i emner (ϕϕ).
    Til sidst stabiliserer disse fordelinger sig omkring værdier, der passer bedst med data.

Så Dirichlet-fordelingen hjælper algoritmen med at finde realistiske og stabile fordelinger af ord og emner.

Dirichlet-fordelingen bruges i LDA, fordi den er specielt designet til fordelinger over proportioner (der summerer til 1).
α-parametrene bestemmer graden af sparsommelighed eller ligelighed i fordelingerne.
I praksis fungerer Dirichlet-fordelingen som en startantagelse, der opdateres iterativt i LDA-processen, hvilket fører til mere realistiske og meningsfulde emner.

Hvorfor Dirichlet-fordelingen bruges i LDA (Intuition)

I LDA modellerer vi sandsynligheder eller proportioner, fx:

    Dokumenter indeholder forskellige emner med forskellige proportioner.
    Emner indeholder forskellige ord med forskellige proportioner.

Disse proportioner skal opfylde følgende:

    Alle proportioner er positive eller nul.
    Alle proportioner skal summere til 1 (fx 40% politik + 30% sport + 30% økonomi = 100%).

Dirichlet-fordelingen er netop designet til at modellere denne slags proportioner, fordi den definerer en sandsynlighedsfordeling over proportioner, der summerer til én.

Dirichlet-fordelingen er en sandsynlighedsfordeling over vektorer af proportioner. Matematikken:

En tilfældig vektor θ=(θ1,θ2,…,θK) følger en Dirichlet-fordeling med parametre α=(α1,α2,…,αK), hvis sandsynlighedstæthedsfunktionen (PDF) er givet ved:

$$
f(\theta; \alpha) = \frac{\Gamma\left(\sum_{i=1}^{K}\alpha_i\right)}{\prod_{i=1}^{K}\Gamma(\alpha_i)} \prod_{i=1}^{K}\theta_i^{\alpha_i - 1}
$$

Hver parameter αiαi​ kan tænkes som en "prior count" eller ”forventet vægt” på kategori/emne ii:

    Lav α (fx α < 1):
        Sandsynlighedsfordelingen koncentreres omkring få emner.
        Resultatet: ét eller få emner dominerer (sparsom fordeling).

    α omkring 1 (fx α = 1):
        Emnerne er omtrent ens sandsynlige.
        Resultatet: Mere tilfældige og jævne emnefordelinger.

    α højere end 1 (fx α > 1):
        Mere jævnt fordelt på tværs af alle emner.
        Resultatet: Dokumenter har ofte en mere ligelig blanding af mange emner.

Kort sagt, α styrer altså, hvor meget variation og sparsommelighed der er i fordelingerne.

LDA bruger Dirichlet-fordelingen to steder:

    Dokument-emne-fordelingen (θθ):
    Hvert dokument dd vælger en fordeling over emner fra en Dirichlet-fordeling:
    θd​∼Dirichlet(α)

    Emne-ord-fordelingen:
    Hvert emne vælger en fordeling over ord fra en Dirichlet-fordeling med parameter ββ:
    ϕk​∼Dirichlet(β)

I praksis betyder dette, at LDA starter med:

    At gætte en tilfældig blanding af emner i hvert dokument (styret af αα).
    At vælge en tilfældig sammensætning af ord inden for hvert emne (styret af ββ).



Forestil dig, at vi har 3 emner (fx Politik, Sport, Økonomi):

Lad os vælge forskellige α-parametre:
Eksempel A (α = [1, 1, 1])

    Emnerne er lige sandsynlige.
    Sandsynlige blandinger:
    (0.33, 0.33, 0.33), (0.4, 0.3, 0.3), (0.3, 0.3, 0.4) osv.

Eksempel B (α = [0.1, 0.1, 0.1])

    Meget sparsomme fordelinger.
    Resultatet vil typisk være: [0.95, 0.04, 0.01] – ét emne dominerer helt.

Eksempel C (α = [10, 10, 10])

    Meget jævne fordelinger: [0.34, 0.33, 0.33].
    Ingen emner er klart dominerende, vi får en jævn fordeling.

```{python}
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import dirichlet

# α-parametre (prøv at ændre disse!)
alpha = [1, 1, 1]

# Generer data fra Dirichlet-fordelingen
samples = dirichlet.rvs(alpha, size=5000)

# Visualisering
fig = plt.figure(figsize=(7, 6))
ax = fig = plt.figure().add_subplot(projection='3d')
ax = plt.axes(projection='3d')

ax.scatter(samples[:,0], samples[:,1], samples[:,2], c='green', marker='o', alpha=0.3)
ax.set_xlabel('Emne 1')
ax.set_ylabel('Emne 2')
ax.set_zlabel('Emne 3')

plt.title('Dirichlet-fordeling med α = [1, 1, 1]')
plt.show()

```

## Fordele og ulemper: opsummering 



Strengths of LDA
Clear Probabilistic Interpretation:

    LDA's explicit probabilistic formulation makes it interpretable and easy to understand mathematically.

Simple Implementation and Efficiency:

    Fast, mature implementations (e.g., Gensim, MALLET, Scikit-learn) are readily available.
    Scales well to large datasets with Gibbs sampling or variational inference.

Robustness and Flexibility:

    Effective even with relatively small datasets.
    Clear mechanisms (via priors αα and ββ) to control topic sparsity.

Human-readable Outputs:

    Topics represented as easily interpretable distributions over words.

Weaknesses of LDA
Bag-of-Words Limitation:

    LDA disregards word order and semantics, relying solely on word frequencies and co-occurrences.
    Cannot distinguish nuanced semantic differences or polysemy (words with multiple meanings).

Sensitive to Hyperparameters and Number of Topics:

    Choosing the number of topics KK and hyperparameters αα and ββ often involves trial-and-error or heuristics.

Difficulties Handling Short Texts:

    Documents with fewer words (e.g., tweets) offer less statistical evidence, resulting in poorer topic quality.

Context Insensitivity:

    Unable to leverage linguistic or semantic context. For example, "apple" (fruit vs. company) is ambiguous in LDA.


# LDA i praksis